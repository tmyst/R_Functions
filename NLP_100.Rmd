---
title: "NLP_100"
author: "tmyst"
date: "2016/6/7"
output:
  html_document:
    css: style.css
    theme: cerulean
---
<style type="text/css">
body, td, th {
   font-size: 14px;
   font-family: "Yu Gothic";
}
h1,h2 {
   font-size: 20px;
   font-family: "Yu Gothic";
}
h3,h4 {
   font-size: 18px;
   font-family: "Yu Gothic";
}
h5,h6 {
   font-size: 16px;
   font-family: "Yu Gothic";
}
code {
  font-size: 14px;
  font-family: MyricaM M;
}
pre {
  font-size: 14px;
  font-family: "Yu Gothic";
}
</style>
---

```{r setup, include=FALSE}
# # knitr::opts_chunk$set(echo = TRUE)
# # devtools::install_github("renkun-ken/formattable")
# # library(formattable)
# Sys.getlocale()
```

```{r}
# Library Setup
library(readxl)
library(readr)
library(stringr)
library(plyr)
library(tidyr)
library(dplyr)
library(ggplot2)
```


```{r}
# -------------------------Natural Language Processing-------------------------
# 0
ch <- "stressed"
str_split("stressed", "")[[1]] %>% rev %>% as.list %>% do.call(paste0, .)
# Using do.call
str_split("stressed", "")[[1]] %>% rev %>% paste(collapse="")
# A little shorter ver.
sapply(lapply(strsplit("stressed", NULL), rev), paste, collapse="")
# A bit complicated
```

```{r}
# 1
ch <- "パタトクカシーー"
extract_ix <- c(1, 3, 5, 7)
paste(str_split(ch, "")[[1]][exrtact_ix], collapse="")
```

```{r}
# 2
ch1 <- "パトカー"
ch2 <- "タクシー"

strlist <- list(ch1, ch2)
chmat <- matrix("", max(sapply(strlist, function(x)str_length(x))), length(strlist))

for(i in 1:length(strlist)){
  chmat[1:str_length(strlist[[i]]), i] <- c(str_split(strlist[[i]], ""))[[1]]
}
chmat %>% t %>% as.vector %>% paste(collapse="")

```

```{r}
# 3
sent <- "Now I need a drink, alcoholic of course, after the heavy lectures involving quantum mechanics."
lapply(str_split(sent, " "), function(x)str_length(gsub(x, pattern="(\\.|\\,)", replacement="", perl=T)))[[1]]
sprintf("%1.14f", pi)

```


```{r}
# 4
sent <- "Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can."
strg <- str_split(sent, " ")[[1]]

char1_ix <- c(1, 5, 6, 7, 8, 9, 15, 16, 19)
char2_ix <- setdiff(1:length(strg), char1_ix)

char1_st <- sapply(strg[char1_ix], function(x)str_split(x, "")[[1]][1])
char2_st <- sapply(strg[char2_ix], function(x)paste(str_split(x, "")[[1]][1:2], collapse=""))

c(paste(char1_st, char1_ix, sep = ":"), paste(char2_st, char2_ix, sep = ":")) %>% sort

```

```{r}
# 5
# for none-0 length sentence
# char_ngram:
#   char::sentence, int::n -> data.frame(char, char)
# word_ngram:
#   char::sentence, int::n -> data.frame(char, char)
# note:position returned by word_ngram is character-wise, not word-wise
sent <- "Hi He Lied Because Boron Could Not Oxidize Fluorine. New Nations Might Also Sign Peace Security Clause. Arthur King Can."

char_ngram <- function(sent, n){
  sent_trim <- lapply(str_split(sent, " "), function(x)str_replace(x, pattern="(\\s)", replacement=""))[[1]] %>% paste(collapse="")

  sent_trim_split <- str_split(sent_trim, pattern = "")[[1]]
  sent_list <- list()
  for(i in 1:n){
    sent_list[[i]] <- sent_trim_split %>% lead(i-1)
  }
  chars <- do.call(paste0, sent_list)[1:(length(sent_trim_split)-n+1)] %>% unique
  pos_list <- sapply(chars, function(x)str_locate_all(sent, x))
  pos <- sapply(pos_list, function(x)ifelse(length(x)!=0, paste(x[,1], collapse=", "), NA))
  
  data.frame(chars, pos)
}

word_ngram <- function(sent, n){
  sent_split <- str_split(sent, " ")[[1]]
  sent_list <- list()
  for(i in 1:n){
    sent_list[[i]] <- sent_split %>% lead(i-1)
  }
  words <- do.call(paste, sent_list)[1:(length(sent_split)-n+1)] %>% unique
  pos_list <- sapply(words, function(x)str_locate_all(sent, x))
  pos <- sapply(pos_list, function(x)ifelse(length(x)!=0, paste(x[,1], collapse=", "), NA))
  data.frame(words, pos)
}

char_ngram(sent, 2)
word_ngram(sent, 2)

```

```{r}
# 6
ch1 <- "paraparaparadise"
ch2 <- "paragraph"
ch1_cbi <- char_ngram(ch1, 2)
ch2_cbi <- char_ngram(ch2, 2)

union(ch1_cbi[[1]], ch2_cbi[[1]])
intersect(ch1_cbi[[1]], ch2_cbi[[1]])
setdiff(ch1_cbi[[1]], ch2_cbi[[1]])
setdiff(ch2_cbi[[1]], ch1_cbi[[1]])

'se' %in% ch1_cbi[[1]]
'se' %in% ch2_cbi[[2]]
```


```{r}
# 7
print_temperature <- function(x, y, z){
  paste0(x, "時の", y, "は", z)
}

x <- 12
y <- "気温"
z <- 22.4
print_temperature(x, y, z)

```


```{r}
# 8
cipher <- function(string){
  lapply(str_split(ch, "")[[1]], function(x){
    ifelse(x %in% letters, rawToChar(as.raw(219-which(x==letters))), x)}) %>% 
  paste(collapse="")
}
ch <- "Hello, World"
cipher(ch)

```


```{r}
# 9
# now complicated
sent <- "I couldn't believe that I could actually understand what I was reading : the phenomenal power of the human mind ."

Typoglycemia <- function(sentence){
  str_split(sentence, " ")[[1]] %>% sapply(., str_split, pattern="") %>% 
  sapply(., function(x){
    ifelse(length(x)>4, 
           paste0(x[1], paste(sample(x[2:(length(x)-1)], length(x)-2), collapse=""), x[length(x)]), 
           paste(x, collapse=""))})
}

Typoglycemia(sent)

```

### From practice no.10 - practice no.19, I use R instead of UNIX shell

```{r}
# 10
ht <- read_tsv("hightemp.txt")
length(ht)

```

```{r}
# 11
# "encoding="UTF-8"" needed when default text encoding=utf-8 on Rstudio windows ver.
ht <- scan("hightemp.txt", what = character(), sep = "\n", blank.lines.skip = F, encoding="UTF-8")
gsub(ht, pattern="\\t", replacement=" ")

```

```{r}
# 12
# "fileEncoding="UTF-8"" needed when default text encoding=utf-8 on Rstudio windows ver.
ht <- read_tsv("hightemp.txt")
write.table(ht[1], "col1.txt", quote=F, sep="\t", row.names=F, col.names=T, fileEncoding="UTF-8")
write.table(ht[2], "col2.txt", quote=F, sep="\t", row.names=F, col.names=T, fileEncoding="UTF-8")

```

```{r}
# 13
col1 <- read_tsv("col1.txt")
col2 <- read_tsv("col2.txt")
data.frame(col1, col2) %>% write.table("col1-2.txt", quote=F, sep="\t", row.names=F, col.names=T, fileEncoding="UTF-8")

```

```{r}
# 14


```

```{r}
# 15


```

```{r}
# 16


```

```{r}
# 17


```

```{r}
# 18


```

```{r}
# 19


```


```{r}
# 20
library(jsonlite)
fn <- "jawiki-country.json.gz"

if (file.exists(file = fn)) {
  dat_gz <- gzfile(description = fn, 
  open = "rb", 
  encoding = "UTF-8")
} else {
  stop("File not found.") 
}

search_res <- read_lines(file = dat_gz, n_max = -1) %>% 
  lapply(X = ., FUN=function(x){
      parsed_json <- jsonlite::fromJSON(txt=x)
      if (is.element(parsed_json$title, iconv("イギリス", from="cp932", to="UTF-8"))){
        return(parsed_json$text)
      } else {
        return(NULL)
      }
    }
  ) %>%
  unlist %>% 
  readr::read_lines(n_max = -1)

close(dat_gz)

```

```{r}
# 21


```

```{r}
# 22


```

```{r}
# 23


```

```{r}
# 24


```

```{r}
# 25


```

```{r}
# 26


```
